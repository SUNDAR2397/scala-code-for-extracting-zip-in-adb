%scala
import scala.sys.process._
import java.nio.file.{Files, Paths, Path}
import scala.collection.JavaConverters._

def readProcessedFilesLog(logPath: String): Set[String] = {
  try {
    dbutils.fs.head(logPath, 10000).split("\n").toSet // Adjust the byte limit if needed
  } catch {
    case _: Exception => Set.empty[String]  // If log file doesn't exist, return an empty set
  }
}

def appendToProcessedFilesLog(logPath: String, fileName: String): Unit = {
  dbutils.fs.put(logPath, fileName + "\n", true)  // true for appending to the file
}

def extractZipFromDbfsToDbfs(directoryPathDbfs: String, extractionPathDbfs: String, processedFilesLogPath: String): Unit = {
  val processedFiles = readProcessedFilesLog(processedFilesLogPath)

  // List all zip files in the directory
  val files = dbutils.fs.ls(directoryPathDbfs).filter(file => file.name.endsWith(".zip"))

  // Filter out files that have already been processed
  val newZipFiles = files.filterNot(file => processedFiles.contains(file.name))

  newZipFiles.foreach { file =>
    val zipFileName = file.name
    println(s"Processing new file: ${file.path}")

    val tmpDir = Files.createTempDirectory("zip_extraction").toString

    try {
      // Copy the zip file to the temporary directory
      dbutils.fs.cp(file.path, s"file://$tmpDir/$zipFileName", recurse = false)
      println(s"File copied to temporary directory: $tmpDir")

      // Unzip the file
      val unzipCommand = s"unzip $tmpDir/$zipFileName -d $tmpDir"
      Process(unzipCommand).!!

      // Copy the extracted files, excluding .zip and .crc files and directories
      Files.list(Paths.get(tmpDir)).iterator().asScala
        .filter(path => !path.getFileName.toString.endsWith(".zip") && !path.getFileName.toString.endsWith(".crc") && !Files.isDirectory(path))
        .foreach { filePath =>
          val dbfsPath = s"$extractionPathDbfs/${filePath.getFileName}"
          dbutils.fs.cp(s"file://$filePath", dbfsPath, recurse = false)
          println(s"Copied to DBFS/ABFSS: $dbfsPath")
        }

      // Log the processed file
      appendToProcessedFilesLog(processedFilesLogPath, file.name)
    } catch {
      case e: Exception => println(s"An error occurred during processing of ${file.path}: ${e.getMessage}")
    } finally {
      // Clean up the temporary directory
      dbutils.fs.rm(s"file://$tmpDir", recurse = true)
    }
  }

  if (newZipFiles.isEmpty) {
    println("No new zip files found to process.")
  }
}




from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Initialize SparkSession (In Databricks, this is usually already done for you)
spark = SparkSession.builder.appName("FileProcessing").getOrCreate()

def process_file(file_path):
    # Load the file into a DataFrame
    df = spark.read.csv(file_path, header=True)  # Adjust this based on your file format and options
    
    # Extract filename from the path (if full path is provided)
    filename = file_path.split("/")[-1]
    
    # Add a column with the filename
    df = df.withColumn("filename", lit(filename))
    
    # Perform additional data processing here as needed
    
    return df  # You might want to return, save, or directly analyze the DataFrame

# Example of processing a specific file
file_to_process = "/path/to/your/file.csv"  # Adjust the path to your file
processed_df = process_file(file_to_process)

# You can now perform further analysis, save or display the processed DataFrame

